{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Evaluation Charts Example\n",
    "\n",
    "This notebook demonstrates how to use the hypothesis evaluation chart functionality to visualize evaluation results from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Image, display\n",
    "import base64\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Add the src directory to the path if needed\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the chart generation functionality\n",
    "from HypothesisEvaluatorAgent.evaluation_charts import display_hypothesis_evaluation_chart, display_evaluation_statistics\n",
    "from HypothesisEvaluatorAgent.database_tools import get_hypothesis_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Display Charts in the Notebook\n",
    "\n",
    "This function takes the base64-encoded image from the chart generation function and displays it in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def display_chart(chart_result: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Display a chart in the notebook from the chart result dictionary.\n",
    "    \n",
    "    Args:\n",
    "        chart_result: Dictionary containing chart data with base64-encoded image\n",
    "    \"\"\"\n",
    "    if not chart_result[\"success\"]:\n",
    "        print(f\"Error: {chart_result['message']}\")\n",
    "        return\n",
    "    \n",
    "    # Get the base64-encoded image\n",
    "    img_data = chart_result[\"chart_data\"][\"image_base64\"]\n",
    "    \n",
    "    # Display the image\n",
    "    display(Image(data=base64.b64decode(img_data)))\n",
    "    \n",
    "    # Print chart metadata\n",
    "    print(f\"Chart Type: {chart_result['chart_type']}\")\n",
    "    print(f\"Hypotheses Displayed: {chart_result['hypothesis_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve All Hypothesis Evaluations\n",
    "\n",
    "First, let's retrieve all hypothesis evaluations from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get all hypothesis evaluations\n",
    "evaluation_results = get_hypothesis_evaluations(limit=50)\n",
    "\n",
    "if evaluation_results[\"success\"]:\n",
    "    evaluations = evaluation_results[\"evaluations\"]\n",
    "    print(f\"Retrieved {len(evaluations)} hypothesis evaluations\")\n",
    "else:\n",
    "    print(f\"Error retrieving evaluations: {evaluation_results['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Evaluation Statistics\n",
    "\n",
    "Let's display the mean and standard deviation for each quality score across all evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get evaluation statistics\n",
    "stats_result = display_evaluation_statistics(limit=50)\n",
    "\n",
    "if stats_result[\"success\"]:\n",
    "    # Display statistics\n",
    "    stats = stats_result[\"statistics\"]\n",
    "    print(f\"Statistics for {stats['count']} hypothesis evaluations:\\n\")\n",
    "    \n",
    "    print(\"Mean Scores:\")\n",
    "    for criterion, score in stats[\"average_scores\"].items():\n",
    "        print(f\"  {criterion.capitalize()}: {score}\")\n",
    "    \n",
    "    print(\"\\nStandard Deviations:\")\n",
    "    for criterion, std in stats[\"standard_deviations\"].items():\n",
    "        print(f\"  {criterion.capitalize()}: {std}\")\n",
    "    \n",
    "    print(\"\\nScore Distribution:\")\n",
    "    for range_name, count in stats[\"score_distribution\"].items():\n",
    "        print(f\"  {range_name}: {count} hypotheses\")\n",
    "    \n",
    "    print(f\"\\nHighest Score: {stats['highest_score']}\")\n",
    "    print(f\"Lowest Score: {stats['lowest_score']}\")\n",
    "else:\n",
    "    print(f\"Error retrieving statistics: {stats_result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Statistics Chart\n",
    "\n",
    "Now let's generate a chart showing the mean and standard deviation for each quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"charts\", exist_ok=True)\n",
    "\n",
    "# Generate statistics chart\n",
    "stats_chart = display_evaluation_statistics(\n",
    "    output_path=\"charts/hypothesis_statistics.png\"\n",
    ")\n",
    "\n",
    "# Display the chart if available\n",
    "if stats_chart[\"success\"] and \"chart_data\" in stats_chart:\n",
    "    # Get the base64-encoded image\n",
    "    img_data = stats_chart[\"chart_data\"][\"image_base64\"]\n",
    "    \n",
    "    # Display the image\n",
    "    display(Image(data=base64.b64decode(img_data)))\n",
    "    \n",
    "    # Confirm file was saved\n",
    "    if stats_chart[\"chart_data\"][\"file_path\"]:\n",
    "        print(f\"Chart saved to: {stats_chart['chart_data']['file_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Display Different Chart Types\n",
    "\n",
    "Now let's generate and display different types of charts for the hypothesis evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Radar Chart\n",
    "\n",
    "Radar charts show all 5 criteria scores for each hypothesis in a radar/spider plot. This is useful for visualizing the strengths and weaknesses of individual hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate radar chart\n",
    "radar_chart = display_hypothesis_evaluation_chart(chart_type=\"radar\", limit=6)\n",
    "\n",
    "# Display the chart\n",
    "display_chart(radar_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bar Chart\n",
    "\n",
    "Bar charts compare scores across hypotheses for each criterion. This is useful for comparing how different hypotheses perform on specific criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate bar chart\n",
    "bar_chart = display_hypothesis_evaluation_chart(chart_type=\"bar\", limit=8)\n",
    "\n",
    "# Display the chart\n",
    "display_chart(bar_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Heatmap\n",
    "\n",
    "Heatmaps show scores for all hypotheses and criteria in a color-coded grid. This is useful for identifying patterns across multiple evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate heatmap\n",
    "heatmap = display_hypothesis_evaluation_chart(chart_type=\"heatmap\", limit=15)\n",
    "\n",
    "# Display the chart\n",
    "display_chart(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Comparison Chart\n",
    "\n",
    "Comparison charts show overall scores for all hypotheses in a ranked order. This is useful for quickly identifying the best hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate comparison chart\n",
    "comparison_chart = display_hypothesis_evaluation_chart(chart_type=\"comparison\", limit=20)\n",
    "\n",
    "# Display the chart\n",
    "display_chart(comparison_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Hypotheses\n",
    "\n",
    "You can filter the hypotheses to include in the charts using various parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Specific Hypothesis IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate chart for specific hypotheses\n",
    "specific_chart = display_hypothesis_evaluation_chart(\n",
    "    hypothesis_ids=[1, 2, 3],  # Replace with actual hypothesis IDs\n",
    "    chart_type=\"radar\"\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "display_chart(specific_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Score Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate chart for hypotheses with high scores\n",
    "high_score_chart = display_hypothesis_evaluation_chart(\n",
    "    min_overall_score=4.0,  # Only include hypotheses with scores >= 4.0\n",
    "    chart_type=\"comparison\"\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "display_chart(high_score_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics for Filtered Hypotheses\n",
    "\n",
    "You can also get statistics for a filtered set of hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get statistics for high-scoring hypotheses\n",
    "high_score_stats = display_evaluation_statistics(\n",
    "    min_overall_score=4.0,\n",
    "    output_path=\"charts/high_score_statistics.png\"\n",
    ")\n",
    "\n",
    "# Display the chart if available\n",
    "if high_score_stats[\"success\"] and \"chart_data\" in high_score_stats:\n",
    "    # Get the base64-encoded image\n",
    "    img_data = high_score_stats[\"chart_data\"][\"image_base64\"]\n",
    "    \n",
    "    # Display the image\n",
    "    display(Image(data=base64.b64decode(img_data)))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Statistics for {high_score_stats['hypothesis_count']} high-scoring hypotheses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Charts to Files\n",
    "\n",
    "You can save charts to files by providing an output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"charts\", exist_ok=True)\n",
    "\n",
    "# Generate and save chart\n",
    "saved_chart = display_hypothesis_evaluation_chart(\n",
    "    chart_type=\"heatmap\",\n",
    "    output_path=\"charts/hypothesis_heatmap.png\"\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "display_chart(saved_chart)\n",
    "\n",
    "# Confirm file was saved\n",
    "if saved_chart[\"chart_data\"][\"file_path\"]:\n",
    "    print(f\"Chart saved to: {saved_chart['chart_data']['file_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to use the hypothesis evaluation chart functionality to visualize evaluation results from the database. You can use these charts and statistics to gain insights into the quality of your chaos engineering hypotheses and identify areas for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
